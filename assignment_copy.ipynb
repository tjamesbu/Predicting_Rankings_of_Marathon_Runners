{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78019c90-62f6-41f6-ace0-54a4a754e5f6",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "In this course assignment you must build a predictive model to determine what place a runner will come in in a foot race. More specifically, you must predict the place order for all participants running races in the year 2019 which are part of a series (e.g. they have been run annually, or at least once previously) using historical data. **For each race you must predict the integer place ordering for all participants**. You are not predicting the top n finishers, or performance bands where people will finish. Instead, your method is expected to be integrated into a premium feature of an application such as Strava, where historical data and data about the racer (and who is signed up for the race!) could be used to help build a personalized prediction for them.\n",
    "\n",
    "## Framing\n",
    "Through this assignment you will demonstrate your ability to build sophisticated supervised machine learning models, from data manipulation through feature engineering and modelling. This is an authentic dataset, and a real-world problem. You can use whatever modelling method you would like to, and can characterise the problem as a regression, classification, or ordinal prediction problem. There is no particular guidelines you must follow, nor guidance offered in the course _per se_ however, there is plenty of opportunity to ask course staff questions. **It is expected that this assignment will take significant effort**.\n",
    "\n",
    "## About the Data\n",
    "All of the races you are asked to predict outcomes for have a temporal relationship with some race in the past (e.g. they are part of an annual series), and I have included an identifier `sequence_id` to help identify this. The `sequence_id` will be included in all races you need to predict, so you can build race-specific features should you wish to. Races which are in your training set and do not have a `sequence_id` could be used however you might like. There may be some races which have a `sequence_id` in the training set but do not have a `sequence_id` in the holdout set -- this all depends what is offered in a given year!\n",
    "\n",
    "A couple of core concepts are important beyond sequences. First, races have categories, which generally (though doesn't need to) denote the length of the race (e.g. 5k, marathon, etc). I've cleaned this column into a new one, prepending the word `clean` so that a columns such as `category.completed.name` becomes `clean_category.completed.name`. I have left the original data in there for you as well, and the transformations I've done have been largely to reduce dimensionality along lines I think is reasonable.\n",
    "\n",
    "In addition to a category, there are `brackets`. Brackets typical denote demographic aspects of the runners and group them, such as Men aged 40-45. I have removed bracket information from the data and instead want you to focus on overall prediction which merges all runners in a given category together. This (should) line up with the rank order based on the individual's time, though I have not verified it (and predicting time is **not** the task).\n",
    "\n",
    "## Evaluation Criteria\n",
    "In this assignment you will be penalized equally for incorrect predictions weighted by the distance by which you are incorrect within a given race. It does not matter whether you over or under predicted a given rank, you are penalized one point per position you are off for a given individual. All DNF's are removed from the dataset, so each person in the dataset has a rank. You *must* provide a predicted rank for each person however, you may rank multiple people at the same spot if you would like (e.g. ties). The evaluation is for each combination of event and category, so a given event may have a 5 kilometer category, a 1 mile category, and so forth. Only individuals registered for a given event and category combination are included in the `DataFrame` you will be asked to predict for. Each event/category pair is equally weighted, and is scaled by the size of the event. Your overall prediction score will be the sum of all scores across the prediction tasks (e.g. across unique combinations of event and category). The exact scoring function is provided below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c3f33-b100-4486-8fe8-1f9e6b448ec2",
   "metadata": {},
   "source": [
    "## Example Solution\n",
    "The following cell contains an example solution to demonstrate the API which is used for this assignment. In short, you are to create an `sklearn.pipeline.Pipeline` object which you `fit()` on your training data using whatever method you like and serialize it to disk in a file called `pipeline.cloudpickle`. This object will then be reinstantiated in the autograder and evaluated based on the scoring function described above. Please note that the solution below would be a poor one, it is intended **only** to demonstrate the API for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b6421-4bcd-403e-a734-3d8b43c61b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "events=df['event.id'].unique()\n",
    "\n",
    "train_set=events[0:100]\n",
    "test_set=events[100:200]\n",
    "holdout_set=events[200:300]\n",
    "\n",
    "train=df.query(\"`event.id` in @train_set\")\n",
    "test=df.query(\"`event.id` in @test_set\")\n",
    "holdout=df.query(\"`event.id` in @holdout_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a4fb1-fa59-4529-8b32-1e4f57e2a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout=holdout.drop(\n",
    "    columns=['time.end',\n",
    "             'body.results_certificate',\n",
    "             'event.results_posted',\n",
    "             'event.results_posted',\n",
    "             'event.results_certificate',\n",
    "             'event.photos_available',\n",
    "             'event.photos_faces',\n",
    "             'event.photos_social_sharing',\n",
    "             'event.results_searchable',\n",
    "             'corral.id',\n",
    "             'corral.name',\n",
    "             'corral.wave',\n",
    "             'corral.time.close',\n",
    "             'corral.time.start',\n",
    "             'result.duration.chip',\n",
    "             'result.duration.pace',\n",
    "             'result.rankings',\n",
    "             'result.splits',\n",
    "             'result.videos',\n",
    "             'result.finished',\n",
    "             'result.disqualified',\n",
    "             'result.duration'])\n",
    "holdout=df.groupby([\"event.id\",\"clean_categories.name\"]).filter(lambda z: len(z)>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e086dd9-eef0-442e-a943-b5802435bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "\n",
    "# This code simulates the autograder. It is not the full autograder implementation\n",
    "# but shares an API with the autograder. It expects that your fitted pipeline is\n",
    "# submitted with the name pipeline.cloudpickle as demonstrated above. This object\n",
    "# must implement the predict() function. This is done automatically by the sklearn\n",
    "# Pipeline object if the last element of your pipeline is a classifier which has\n",
    "# a predict() function. If you are not submitting a Pipeline, and want to do something\n",
    "# different, you *must* have a predict() function of the same method signature, e.g.:\n",
    "#\n",
    "#   predict(self, X, **predict_params)->np.ndarray\n",
    "\n",
    "# Load holdout data, in this case I'll simulate it by loading the training data\n",
    "df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "\n",
    "# And evaluate on all 5k races that we didn't consider for training\n",
    "holdout_data=df.query(\"`event.id`!='583f013a-1e54-4906-87f7-2b625206f5f9' and `clean_categories.name`=='5k'\")\n",
    "\n",
    "# This is the scoring function to determine model fitness\n",
    "def score(left: pd.DataFrame, right: pd.DataFrame):\n",
    "    '''\n",
    "    Calculates the difference between the left and the right when considering rank of items. \n",
    "    This scoring function requires that the two DataFrames have identical indicies, and that\n",
    "    they each contain only one column of values and no missing values. Props to Blake Atkinson\n",
    "    for providing MWE indicating issues with autograder version #1.\n",
    "    '''\n",
    "    assert(type(left)==pd.DataFrame)\n",
    "    assert(type(right)==pd.DataFrame)\n",
    "    assert(len(left)==len(right))\n",
    "    assert(not np.any(np.isnan(left)))\n",
    "    assert(not np.any(np.isnan(right)))\n",
    "    assert(left.index.equals(right.index))\n",
    "    # convert to ndarrays\n",
    "    left=left.squeeze()\n",
    "    right=right.squeeze()\n",
    "    return np.sum(np.abs(left-right))/(len(left)*(len(left)-1))\n",
    "\n",
    "# This function runs the prediction model agains a given event/category pair. It\n",
    "# intentionally loads the student model each time to avoid accidental leakage of data\n",
    "# between events.\n",
    "def evaluate(data, pipeline_file='pipeline.cloudpickle'):\n",
    "    # Load student pipeline\n",
    "    fitted_pipe = cloudpickle.load(open(pipeline_file,'rb'))\n",
    "    \n",
    "    # Separate out the X and y\n",
    "    X=list(set(data.columns)-{'overall_ranking'})\n",
    "    y=['overall_ranking']\n",
    "    \n",
    "    # Drop any missing results (DNFs)\n",
    "    data=data.dropna(subset=['overall_ranking'])\n",
    "    \n",
    "    # Ensure there is data to actually predict on\n",
    "    if len(data)==0:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict on unseen data\n",
    "    predictions=pd.DataFrame(fitted_pipe.predict(data[X]),data.index)\n",
    "    observed=data[y]\n",
    "    \n",
    "    # Generate rankings within this bracket\n",
    "    observed=pd.DataFrame(data[y].rank(),data.index)\n",
    "    \n",
    "    # Return the ratio of the student score\n",
    "    return pd.Series({\"score\":score(observed,predictions)})\n",
    "\n",
    "# Student solution\n",
    "pipeline_file='pipeline.cloudpickle'\n",
    "\n",
    "def autograde(data_held_out):\n",
    "    # Run prediction on each group\n",
    "    results=data_held_out.groupby([\"event.id\",\"clean_categories.name\"]).apply(evaluate, pipeline_file)\n",
    "\n",
    "    # Display the results, uncomment this for your own display\n",
    "    results.reset_index()['score'].plot.bar();\n",
    "\n",
    "    # This is the student final grade\n",
    "    print(np.average(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f41b3-4e1c-45e2-b656-1985a90b522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.to_timedelta(train['result.duration.chip']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa733e3c-fc24-4c00-a92b-41da6689acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "def roll_own()->object:\n",
    "    \"\"\"This function returns a fitted object with a predict(x) function\"\"\"\n",
    "    \n",
    "    # First I'm going to create a new class with a predict function, and that class\n",
    "    # is just going to call the regression object it is setup with and then rerank\n",
    "    # all of the values which come back\n",
    "    class RollingRegressor():\n",
    "        \n",
    "        # For this class I'm going to assume it has been given a fitted model, so\n",
    "        # I'm choosing not to implement the fit() function.\n",
    "        def __init__(self, fitted_model):\n",
    "            self.regressor=fitted_model\n",
    "        \n",
    "        # For the prediction we are just given our dataframe, so we have to do our\n",
    "        # data cleaning here.\n",
    "        def predict(self, X):\n",
    "            # We need to be careful and *not* drop rows. The autograder is expecting\n",
    "            # a rank back for every row in X! Lets just grab out the two columns of\n",
    "            # interest\n",
    "            df=X[[\"age\",\"sex\"]]\n",
    "            \n",
    "            # For brevity let's get rid of any sex that isn't Male/Female and replace with nan\n",
    "            # A better approach would be to inspect and map this data accordingly, but I'll leave\n",
    "            # that as an enhancement.\n",
    "            #df.loc[df.query(\"`sex` not in ['Male','Female']\").index, 'sex']=np.nan\n",
    "            \n",
    "            # Now that this is binary we can convert this column into a numeric value\n",
    "            df.loc[ df['sex'] == 'F', 'sex'] = 'Female'\n",
    "            df.loc[ df['sex'] == 'M', 'sex'] = 'Male'\n",
    "            df.loc[df.query(\"`sex` not in ['Male','Female']\").index, 'sex']=np.nan\n",
    "            \n",
    "            df.loc[df.query(\"`sex` == 'Male'\").index, 'sex']=2\n",
    "            df.loc[df.query(\"`sex` == 'Female'\").index, 'sex']=1\n",
    "            \n",
    "            # Now just do whatever you want with missing values, this below doesn't seem ideal\n",
    "            df=df.fillna(0)\n",
    "            \n",
    "            # With the data cleaning done, we can now predict the times for our data\n",
    "            times=self.regressor.predict(df)\n",
    "            \n",
    "            # We can't return the times directly - the autograder wants ranks. We can\n",
    "            # use a similar method those to return ranks\n",
    "            return times.squeeze().argsort()+1\n",
    "        \n",
    "    # Our return class is done, now we just need to initalize it with a fitted\n",
    "    # model. To fit the model we just do all of the cleaning over, and add in some training.\n",
    "    # It would be a better ideal to put this all in the class itself, but I want to\n",
    "    # show you that this isn't needed -- the autograder is NOT going to try and fit()\n",
    "    # your model, it is only going to call predict(), so you can do whatever you want\n",
    "    # within that predict()\n",
    "    \n",
    "    df=train[[\"age\",\"sex\"]]\n",
    "    #df.loc[df.query(\"`sex` not in ['Male','Female','Unspecified']\").index, 'sex']=np.nan\n",
    "    #df.loc[df.query(\"`sex` == 'Male'\").index, 'sex']=2\n",
    "    #df.loc[df.query(\"`sex` == 'Female'\").index, 'sex']=1\n",
    "    #df.loc[df.query(\"`sex` == 'Unspecified'\").index, 'sex']=0\n",
    "    #df=df.fillna(0)\n",
    "    \n",
    "    # Since we have decided it's a regression problem, we can decide to use a simple linear\n",
    "    # model for our first attempt too, so I'll create that now\n",
    "    #from sklearn.linear_model import LinearRegression\n",
    "    #reg=LinearRegression()\n",
    "    #reg.fit(df,y)\n",
    "    \n",
    "    # Now we just return the object that the autograder will want\n",
    "    #return RollingRegressor(reg)\n",
    "    return df\n",
    "# We can test this out by instantiating it\n",
    "fitted_reg=roll_own()\n",
    "# Then saving it to a file\n",
    "cloudpickle.dump(fitted_reg, open('pipeline.cloudpickle','wb'))\n",
    "# Then telling the autograder function to fire\n",
    "autograde(holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90998396-6438-4533-9677-236cd6b7a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# This is a custom transformer to demonstrate how you might modify the data for feature\n",
    "# selection or engineering before applying a given model. In this example I am only\n",
    "# doing feature selection, and passing to the next element in the pipeline the age\n",
    "# and bib number for the runner. Thus only two features will be used in my predictive model.\n",
    "# There are other ways to do this\n",
    "class CustomTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Just select the features we want\n",
    "        Xprime=X[['age','bib',]]\n",
    "        # Ensure that they have numbers in them of the regression will fail\n",
    "        Xprime=Xprime.fillna(value=-1)\n",
    "        return Xprime\n",
    "\n",
    "# I build a very basic pipeline which is made up of three stages. In the first, my\n",
    "# custom transform is called and reduces the DataFrame to just two columns. In the\n",
    "# second I use a built-in transformer from sklearn to bucket users based on their\n",
    "# bib number, perhaps as a proxy for \"how early did they sign up\". In the final step\n",
    "# I want to use a LinearRegression() regressor.\n",
    "\n",
    "# There are two main concerns I need to address. First, I need to be resilient to bad\n",
    "# data which might address. So I know the LinearRegression() object can't handle\n",
    "# missing data, so I need to deal with that. This was done in the CustomeTransformer()\n",
    "# already.\n",
    "\n",
    "# Second, I actually need to be ranking results, not regressing. Depending upon your\n",
    "# model you need to consider this carefully. Here is a fine catch all if you\n",
    "# are using regression, and object which just ranks the results in order. This is\n",
    "# called monkey patching and replaces the LinearRegression() object's predict()\n",
    "# function with a wrapper\n",
    "\n",
    "#reg=LinearRegression()\n",
    "reg=RandomForestRegressor()\n",
    "reg.original_predict=reg.predict\n",
    "\n",
    "def new_predict(X):\n",
    "    # run the old regression method\n",
    "    rankings=reg.original_predict(X)\n",
    "    # now calculate and return the ranks of each item instead\n",
    "    # we need to add a +1 because the lowest rank is a 1, not a 0\n",
    "    # it's unfortunate, the first athletic competition was probably run by R users...\n",
    "    return rankings.squeeze().argsort()+1\n",
    "\n",
    "# Now we overwrite (monkey patch) the predict() function with our own implementation\n",
    "reg.predict=new_predict\n",
    "\n",
    "# And build our pipeline object\n",
    "pipe = make_pipeline( CustomTransformer(), QuantileTransformer(), reg )\n",
    "\n",
    "# This is just one way to do this, you could also implement a new estimator with the\n",
    "# predict interface and build all of your logic in there. The benefit of the\n",
    "# pipeline is that you can rapidly change the logic and try different pipelines using\n",
    "# common methods from sklearn. When the pipeline gets complicated, you can also\n",
    "# visualize it...\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "display(pipe)\n",
    "\n",
    "# Once the pipeline is built, we need to train it. I'm going to just do a pretty poor\n",
    "# job here, getting the training set provided\n",
    "df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "\n",
    "# I'm just going to build a model off of one event/category combination (lame)\n",
    "training_data=df.query(\"`event.id`=='583f013a-1e54-4906-87f7-2b625206f5f9' and `clean_categories.name`=='5k'\")\n",
    "\n",
    "# And I'm going to pass in all of my potential columns for consideration. Note: The\n",
    "# example pipeline I built is going to reduce this to just the two columns I'm interested\n",
    "# in, so this is a safe thing to do. But be aware, the holdout set does not have all of\n",
    "# the data the training set might, because of leakage, so you need to think about this\n",
    "# and not make assumptions. You can see how I built the holdout set at the bottom of\n",
    "# this notebook\n",
    "X=set(training_data.columns)-{'overall_ranking'}\n",
    "\n",
    "# The ranking is what we aim to predict\n",
    "y={'overall_ranking'}\n",
    "\n",
    "# Now I fit() the pipeline. You'll note that the outcomes I need to squeeze() to ensure\n",
    "# it's a one dimensional structure and not a DataFrame\n",
    "fitted_pipe=pipe.fit(training_data[X],training_data[y].squeeze().to_numpy())\n",
    "\n",
    "# And now, assuming that I am happy with this model and think it is great, I write the\n",
    "# fitted pipeline to a file. This file will be read in by the autograder.\n",
    "cloudpickle.dump(fitted_pipe, open('pipeline.cloudpickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91ab8c-fc6c-4c7a-85e5-90119cc1e063",
   "metadata": {},
   "source": [
    "## Testing the Solution\n",
    "With a minimum pipeline built we can think about testing it. The code below simulates the autograder, and is something you can use to evaluate how your model performs. The most important function is the `score()` function, which demonstrates how the score of the model fitness will be determined, as described previously. This function just compares two ranked lists and determined how aligned they are with one another. The second function is the `evaluate()` function, which runs your model over a given race of data. Note that the evaluation generates new ranks from the `overall_ranking` but doesn't use those numbers directly. Those numbers are in-order, but due to underlying data assumptions may have gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222151c-a5ea-425c-b7a9-b5b29d9f410b",
   "metadata": {},
   "source": [
    "# Addendum\n",
    "The following is the code I wrote to clean the data once it had been obtained. This may or may not be useful in helping to explain the structure of the data to you. You should leverage the visual exploration techniques and supervised machine learning techniques to better understand this data. **You do not have to run this file, this file does not do anything as I've already preprocessed the data for you.** The rest of this file is simply to give you a better idea of the task, and the code should demonstrate how I've done some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974f7fb-7d04-4c21-9db5-4da885015a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# All of the runners\n",
    "df_runners=pd.read_csv(\"../runners/full_runners.csv\")\n",
    "\n",
    "# All of the races\n",
    "df_races=pd.read_csv(\"../processed_historical_races.csv\")\n",
    "\n",
    "# annoying capitalization in category names\n",
    "df_races[\"clean_categories.name\"]=df_races[\"categories.name\"].dropna().apply(lambda x: x.lower().strip())\n",
    "\n",
    "# make this change in the runners too\n",
    "df_runners[\"clean_category.completed.name\"]=df_runners[\"category.completed.name\"].dropna().apply(lambda x: x.lower().strip())\n",
    "df_runners[\"clean_category.registered.name\"]=df_runners[\"category.registered.name\"].dropna().apply(lambda x: x.lower().strip())\n",
    "\n",
    "# remove cruft left over from poor scraping\n",
    "del(df_races[\"Unnamed: 0\"])\n",
    "del(df_runners[\"Unnamed: 0\"])\n",
    "\n",
    "# deal with column name conflicts, the id==body.id for a race which had details so lets use the latter as it is unique\n",
    "df_races['body.id']=df_races['id']\n",
    "del(df_races['id'])\n",
    "\n",
    "# Prune away all data not in 2019 or lower\n",
    "df_runners=df_runners[df_runners['event.date.start']<'2020-01-01']\n",
    "\n",
    "# Data processing error means some people are in the frame twice, so we need to priune dupe runners\n",
    "df_runners=df_runners.groupby([\"event.id\",\"clean_category.registered.name\",\"registrant_id\"]).head(1)\n",
    "\n",
    "# Create a merged dataframe\n",
    "df=(df_races.query('`brackets.name` in [\"Overall\"]').dropna(subset=['clean_categories.name'])\n",
    "    .merge(df_runners, how='inner',left_on=['body.id','clean_categories.name'],right_on=['event.id','clean_category.registered.name'])\n",
    "    .reset_index(drop=True)\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbf6aa-d0a7-478e-9f39-45513c38a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate sequences we should do so by looking at the lineage.previous_event.id\n",
    "import numpy as np\n",
    "\n",
    "# a sequences starts at a root, which is a race which has an na lineage.previous_event.id\n",
    "root_ids=df[(df['lineage.previous_event.id'].isna())]['body.id'].dropna().unique()\n",
    "\n",
    "# place for sequence identifiers\n",
    "df['sequence_id']=np.nan\n",
    "\n",
    "# our strategy for labeling sequences is to set sequence_id to the root node's body.id\n",
    "for root in root_ids:\n",
    "    nodes=[root]\n",
    "    while(True):\n",
    "        # set the sequence_id to whatever the root is\n",
    "        df.loc[df['body.id'].isin(nodes), 'sequence_id']=root\n",
    "        # get all of the children for that that root and put them in nodes\n",
    "        nodes=df.query('`lineage.previous_event.id` in @nodes')['body.id'].dropna().unique()\n",
    "        # check the base case of no children and break out of loop to go to next sequence\n",
    "        if len(nodes)==0:\n",
    "            break\n",
    "\n",
    "# sanity check, there should be as many sequences as there are roots\n",
    "assert len(df['sequence_id'].dropna().unique()) == len(root_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ea6bc-7336-4be5-949b-455eba74fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create the column we aim to predict, the ranking within a race this is buried in a\n",
    "# column that looks kind of like json but isn't\n",
    "# In the end it's unclear that we can completely rely on this, and we need to just\n",
    "# use this data to generate a ranked list from. This is likely the result of incomplete\n",
    "# scraping efforts.\n",
    "def find_rank(lst,name='Overall'):\n",
    "    # where did this data come from? not json? not pickle?\n",
    "    lst=json.loads(lst.replace(\"'\",'\"'))\n",
    "    for dct in lst:\n",
    "        try:\n",
    "            if dct['name']==name:\n",
    "                return \"\".join(filter(str.isdigit, dct['rank']))\n",
    "        except:\n",
    "            return np.nan\n",
    "df['overall_ranking']=df['result.rankings'].apply(find_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555752f-41cd-4c1d-b729-8e67768ef9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to separate out the races from 2019 to put in our holdout set\n",
    "# First verify that we don't have to worry about multiple races happening in 2019\n",
    "# like a monthly race or something\n",
    "assert 1 == df.query('`event.date.start`>\"2019-01-01\"').groupby(['sequence_id','clean_categories.name']).apply(lambda x: len(x['body.id'].unique())).sort_values(ascending=False).head(1).iloc[0]\n",
    "\n",
    "# Pull out all of the observations which have a sequence id in the year 2019 and this \n",
    "# is our holdout set. This means a given predictive group might be a new category we've \n",
    "# never seen before in this race, e.g. a new 5k track for and event that has run many \n",
    "# times, or that it might be an old category which we've seen lots of.\n",
    "df_holdout=df[(df['event.date.start']>='2019-01-01') & (df['sequence_id'].notna())]\n",
    "\n",
    "# our training set is every race which is not in 2019\n",
    "df_train=df[df['event.date.start']<'2019-01-01']\n",
    "\n",
    "# Discard all races in the holdout set which are not in a sequence in our training set,\n",
    "# as these are new races and we don't expect them to be predicted\n",
    "df_holdout=df_holdout[df_holdout['sequence_id'].isin(df_train['sequence_id'].dropna().unique())]\n",
    "\n",
    "# Now, we have to remove all data which wouldn't be known when the gun fires at the\n",
    "# race from our holdout dataset. We won't remove this from the student datafiles, but\n",
    "# since they know it's not in the holdout I'm sure they will do the right thing and not\n",
    "# try and predict on it\n",
    "df_holdout=df_holdout.drop(\n",
    "    columns=['time.end',\n",
    "             'body.results_certificate',\n",
    "             'event.results_posted',\n",
    "             'event.results_posted',\n",
    "             'event.results_certificate',\n",
    "             'event.photos_available',\n",
    "             'event.photos_faces',\n",
    "             'event.photos_social_sharing',\n",
    "             'event.results_searchable',\n",
    "             'corral.id',\n",
    "             'corral.name',\n",
    "             'corral.wave',\n",
    "             'corral.time.close',\n",
    "             'corral.time.start',\n",
    "             'result.duration.chip',\n",
    "             'result.duration.pace',\n",
    "             'result.rankings',\n",
    "             'result.splits',\n",
    "             'result.videos',\n",
    "             'result.finished',\n",
    "             'result.disqualified',\n",
    "             'result.duration'])\n",
    "\n",
    "df_holdout.to_csv(\"df_holdout.csv.gz\",index=False,compression='gzip')\n",
    "df_train.to_csv(\"df_train.csv.gz\",index=False,compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b366b-a340-4c5c-a0be-f080714bbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just summarize a few things about the training set:\n",
    "print(f\"There are {len(df_train)} observations in the training data.\")\n",
    "print(f\"There are {len(df_train.groupby(['clean_categories.name','sequence_id']).groups)} groups of categories and sequences in the training data.\")\n",
    "print(f\"There are {len(df_train['user_id'].unique())} unique users in the training data.\")\n",
    "print(f\"There are {len(df_train[df_train['user_id'].isna()])} users without a user id in the training data.\")\n",
    "print(f\"There are {len(df_train[df_train['age']==43])} 43 year old runners in the training data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

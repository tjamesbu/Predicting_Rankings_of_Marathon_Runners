{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53d7bc-9a3c-40b7-b8cf-008f142edc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "preprocessor = StandardScaler()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score, KFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "#df = df['overall_ranking'].dropna()\n",
    "events=df['event.id'].unique()\n",
    "\n",
    "df.loc[df.query(\"`sex` not in ['Male','Female']\").index, 'sex']=0\n",
    "\n",
    "df.loc[df.query(\"`sex` == 'Male'\").index, 'sex']=2\n",
    "df.loc[df.query(\"`sex` == 'Female'\").index, 'sex']=1\n",
    "\n",
    "df['sex'] = df['sex'].astype(int)\n",
    "\n",
    "   \n",
    "df['clean_categories_run_count']=df['clean_categories.name'].replace({'15k':15, '5k':5, 'marathon':42.195, 'half marathon':21.0975, '10k':10, '5 mile':8.04672,\n",
    "'8k':8,'5k run':5, '3k walk':0, '10k run':10, 'youth mile':1.60934, '5k run/walk':5,\n",
    "       '5k walk':0, '5 mile run':8.04672, '1 mile run':1.60934, '15k run':15, '10k walk':0,\n",
    "       '6k run':6, '6k walk':0, '1 mile walk':0, '8k run':8, '8k walk':0,\n",
    "       '10 mile run':16.0934, '10 mile walk':0, '5 mile walk':0, '30k run':30,\n",
    "       '30k bike':0, '8 mile run':12.8748, '8 mile walk':0, 'bridge run':10,\n",
    "       'international half marathon':21.0975, 'u.s. only half marathon':21.0975, '1 mile':1.60934,\n",
    "       '5.7 mile run':9.17326, '5.7 mile walk':0, '5k competitive walk':0,\n",
    "       'half marathon run':21.0975, '5k fun walk':0, '5k wheelchair':0,\n",
    "       '1 mile fun run':1.60934, '2 mile run':3.21869, '2 mile walk':0, '4 mile run/walk':6.43738,\n",
    "       'life time commitment day 5k':5, 'midnight streak':5,\n",
    "       'commitment day 5k - master':5, 'quarter marathon':10.54875, '5k walk/run':5,\n",
    "       '1 mile fun run/walk':1.60934, 'one mile fun run':1.60934, '5 km run':5,\n",
    "       'fire fighter':42.195, 'olympic triathlon':10, 'sprint triathlon':10,\n",
    "       'olympic duathlon':10, 'sprint duathlon':10, '10k scenic challenge':10,\n",
    "       'run swim run':50, 'half marathon walk':0, 'mini-marathon':21.0975})\n",
    "\n",
    "df['clean_categories_walk_count']=df['clean_categories.name'].replace({'15k':0, '5k':0, 'marathon':0, 'half marathon':0, '10k':0, '5 mile':0,\n",
    "       '8k':0,'5k run':0, '3k walk':3, '10k run':0, 'youth mile':0, '5k run/walk':0,\n",
    "       '5k walk':5, '5 mile run':0, '1 mile run':0, '15k run':0, '10k walk':10,\n",
    "       '6k run':0, '6k walk':6, '1 mile walk':1.60934, '8k run':0, '8k walk':8,\n",
    "       '10 mile run':0, '10 mile walk':16.0934, '5 mile walk':8.04672, '30k run':0,\n",
    "       '30k bike':0, '8 mile run':0, '8 mile walk':12.9748, 'bridge run':0,\n",
    "       'international half marathon':0, 'u.s. only half marathon':0, '1 mile':0,\n",
    "       '5.7 mile run':0, '5.7 mile walk':9.17326, '5k competitive walk':5,\n",
    "       'half marathon run':0, '5k fun walk':5, '5k wheelchair':0,\n",
    "       '1 mile fun run':0, '2 mile run':0, '2 mile walk':3.21869, '4 mile run/walk':0,\n",
    "       'life time commitment day 5k':0, 'midnight streak':0,\n",
    "       'commitment day 5k - master':0, 'quarter marathon':0, '5k walk/run':0,\n",
    "       '1 mile fun run/walk':0, 'one mile fun run':0, '5 km run':0,\n",
    "       'fire fighter':0, 'olympic triathlon':0, 'sprint triathlon':0,\n",
    "       'olympic duathlon':0, 'sprint duathlon':0, '10k scenic challenge':0,\n",
    "       'run swim run':0, 'half marathon walk':21.0975, 'mini-marathon':0})\n",
    "\n",
    "#walk_count = df['clean_categories_walk_count'].unique()\n",
    "#run_count = df['clean_categories_run_count'].unique()\n",
    "\n",
    "df.loc[df.query(\"`age` >= 119\").index, 'age']=np.nan\n",
    "df.loc[df.query(\"`age` <= 18\").index, 'age']=np.nan\n",
    "df.loc[df['age'] == 'nan', 'age'] = 38\n",
    "df['age']=df['age'].replace(np.nan,38)\n",
    "\n",
    "df['price']=df['price'].replace(np.nan,49.0)\n",
    "\n",
    "df['bib']=df['bib'].replace(np.nan,5202.0)\n",
    "\n",
    "df['counts.participants.expected']=df['counts.participants.expected'].replace(['150-300'],['300'])\n",
    "\n",
    "df['location.city']=df['location.city'].replace(['Oklahoma City, OK'],['Oklahoma City'])\n",
    "\n",
    "df['date'] = pd.to_datetime(df['event.date.start'])\n",
    "df['year'], df['month'] = df['date'].dt.year, df['date'].dt.month\n",
    "\n",
    "#df['event.results_certificate']=df['event.results_certificate'].replace([False,'False','1', 1],[0,0,1,1])\n",
    "#df['event.photos_faces']=df['event.photos_faces'].replace([False, 'False', '1', 1, 'True'],[0,0,1,1,1]\n",
    "\n",
    "df['result.primary_bracket']=df['result.primary_bracket'].replace(['Overall', 'Wheelchair', 'nan', 'Male', 'Female', 'Male 25-29',\n",
    "       'Male 40-44', 'Female 25-29', 'Female 20-24', 'wheelchair','WHEELCHAIR'],['Overall','Wheelchair','Overall','Male','Female','Male','Male', 'Female', 'Female','Wheelchair', 'Wheelchair']) \n",
    "df['result.primary_bracket']=df['result.primary_bracket'].replace(np.nan,'Overall')\n",
    "df['result.primary_bracket']=df['result.primary_bracket'].replace(['Wheelchair', 'Overall', 'Male', 'Female'],[1,2,3,4])\n",
    "\n",
    "#df = df.dropna(subset = ['result.duration.chip'])\n",
    "\n",
    "#groups=pd.DataFrame(df.groupby([\"event.id\",\"clean_categories.name\"]).groups.keys(), columns=[\"event.id\",\"clean_categories.name\"])\n",
    "\n",
    "#train_set=events[100:]\n",
    "#holdout_set=events[0:100]\n",
    "\n",
    "#train_set=events[0:100]\n",
    "#test_set=events[100:200]\n",
    "#holdout_set=events[200:300]\n",
    "\n",
    "#train_set = df[(df['year']==2018) | (df['year']==2017)] \n",
    "#holdout_set = df[(df[\"year\"]==2016) | (df[\"year\"]==2015) | (df[\"year\"]==2014) | (df[\"year\"]==2013) | (df[\"year\"]==2012)]\n",
    "\n",
    "#train = df[df['year']==2017] \n",
    "#holdout = df[df['year']==2018] \n",
    "\n",
    "# We are going to regress on time\n",
    "#df['result.duration.chip']=pd.to_timedelta(df['result.duration.chip']).astype(int)\n",
    "\n",
    "# For this demo we are going to use the sequence identifiers, let's look at a\n",
    "# couple I hand picked\n",
    "#data=df.query('sequence_id in [\"5e862221-758c-48b1-a7cf-11bcc0a80a41\",\"57bdcd1f-a474-43e0-8e54-5f3a5206f5f9\"]')\n",
    "#data.groupby(['sequence_id','event.id']).apply(len)\n",
    "\n",
    "train_set = events[:100]\n",
    "holdout_set = events[100:]\n",
    "\n",
    "#test_set=events[200:300]\n",
    "\n",
    "train=df.query(\"`event.id` in @train_set\")\n",
    "#test=df.query(\"`event.id` in @test_set\")\n",
    "holdout=df.query(\"`event.id` in @holdout_set\")\n",
    "\n",
    "# We see that these two sequences have historical event data, and different numbers of runners\n",
    "# so lets separate this into a training and validation set\n",
    "#train=data.query(\"`event.id` in ['57bdcd1f-a474-43e0-8e54-5f3a5206f5f9', '5e862221-758c-48b1-a7cf-11bcc0a80a41']\")\n",
    "#test=data.query(\"`event.id` not in ['57bdcd1f-a474-43e0-8e54-5f3a5206f5f9', '5e862221-758c-48b1-a7cf-11bcc0a80a41']\")\n",
    "\n",
    "#train=train.groupby([\"event.id\",\"clean_categories.name\"]).filter(lambda z: len(z)>5)\n",
    "\n",
    "holdout=holdout.drop(\n",
    "    columns=['time.end',\n",
    "             'body.results_certificate',\n",
    "             'event.results_posted',\n",
    "            'event.results_posted',\n",
    "             'event.results_certificate',\n",
    "             'event.photos_available',\n",
    "             'event.photos_faces',\n",
    "             'event.photos_social_sharing',\n",
    "             'event.results_searchable',\n",
    "             'corral.id',\n",
    "             'corral.name',\n",
    "             'corral.wave',\n",
    "             'corral.time.close',\n",
    "             'corral.time.start',\n",
    "             'result.duration.chip',\n",
    "             'result.duration.pace',\n",
    "             'result.rankings',\n",
    "             'result.splits',\n",
    "             'result.videos',\n",
    "             'result.finished',\n",
    "             'result.disqualified',\n",
    "             'result.duration'])\n",
    "\n",
    "holdout=df.groupby([\"event.id\",\"clean_categories.name\"]).filter(lambda z: len(z)>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250d969-1072-43ca-9e54-a1e5d05709c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_nan = df['result.primary_bracket'].isnull().sum()\n",
    "#check_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ad048-b608-4c0a-92e7-cdb573b32af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['result.primary_bracket'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce5f44-8e4e-4429-9a2a-05f60c4f924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set['age'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0c774-a1bc-4744-bb14-0dc4052d8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaee6da-f832-4dab-824f-3c919fcfe79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b50ac-2770-47e8-8345-7897a1144e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258094e-4130-4b0c-aa3d-a1e8d6f0a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2018].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7b1e6-eb14-479a-acad-0f312622ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2017].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109b6fa-0c04-4b42-bfdf-c2e5fa95bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2016].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9b787-98ca-46f7-9c86-05fee8167991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2015].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea351c-0560-443e-a18a-473251f1f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2014].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155cbd7-9919-4ab0-bc78-ea991e337015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2013].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222269e-330a-48a9-a72f-09d5aa67880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['year']==2012].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958d372-3398-4492-b6f2-b449451def86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups=pd.DataFrame(df.groupby([\"event.id\",\"clean_categories.name\"]).groups.keys(), columns=[\"event.id\",\"clean_categories.name\"])\n",
    "#groups=pd.DataFrame(df.groupby([\"event.id\",\"clean_categories.name\"]).apply(\n",
    "#groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c5ba7-52a7-4202-955c-eef1ec2f7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_columns', None):\n",
    "#    display(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c468c-3733-433c-b64d-7ee531b0ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_nan = df['clean_categories.name'].isnull().sum()\n",
    "#check_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc2e11-26be-4349-88cc-39beb416d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df['overall_ranking'].isna()==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed6c80-8f62-47fb-9684-bad3ff0999d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.to_timedelta(train['result.duration.chip']).astype(int)\n",
    "#y=train['overall_ranking']\n",
    "holdout_data=holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cff48a-08f4-48f2-bf72-38fce20f20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use the autograder!\n",
    "\n",
    "# I gave you the autograder code, so a next great step is just to copy and paste\n",
    "# that in your notebook and get used to how it works.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "\n",
    "# This code simulates the autograder. It is not the full autograder implementation\n",
    "# but shares an API with the autograder. It expects that your fitted pipeline is\n",
    "# submitted with the name pipeline.cloudpickle as demonstrated above. This object\n",
    "# must implement the predict() function. This is done automatically by the sklearn\n",
    "# Pipeline object if the last element of your pipeline is a classifier which has\n",
    "# a predict() function. If you are not submitting a Pipeline, and want to do something\n",
    "# different, you *must* have a predict() function of the same method signature, e.g.:\n",
    "#\n",
    "#   predict(self, X, **predict_params)->np.ndarray\n",
    "\n",
    "# Load holdout data, in this case I'll simulate it by loading the training data\n",
    "#df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "\n",
    "# And evaluate on all 5k races that we didn't consider for training\n",
    "#holdout_data=df.query(\"`event.id`!='583f013a-1e54-4906-87f7-2b625206f5f9' and `clean_categories.name`=='5k'\")\n",
    "#holdout_data=holdout\n",
    "\n",
    "# This is the scoring function to determine model fitness\n",
    "def score(left: pd.DataFrame, right: pd.DataFrame):\n",
    "    '''\n",
    "    Calculates the difference between the left and the right when considering rank of items. \n",
    "    This scoring function requires that the two DataFrames have identical indicies, and that\n",
    "    they each contain only one column of values and no missing values.\n",
    "    '''\n",
    "    assert(type(left)==pd.DataFrame)\n",
    "    assert(type(right)==pd.DataFrame)\n",
    "    assert(len(left)==len(right))\n",
    "    assert(not np.any(np.isnan(left)))\n",
    "    assert(not np.any(np.isnan(right)))\n",
    "    assert(left.index.equals(right.index))\n",
    "    # convert to ndarrays\n",
    "    left=left.squeeze()\n",
    "    right=right.squeeze()\n",
    "    return np.sum(np.abs(left-right))/(len(left)*(len(left)-1))\n",
    "\n",
    "# This function runs the prediction model agains a given event/category pair. It\n",
    "# intentionally loads the student model each time to avoid accidental leakage of data\n",
    "# between events.\n",
    "def evaluate(data, pipeline_file='pipeline.cloudpickle'):\n",
    "    # Load student pipeline\n",
    "    fitted_pipe = cloudpickle.load(open(pipeline_file,'rb'))\n",
    "    \n",
    "    # Separate out the X and y\n",
    "    X=list(set(data.columns)-{'overall_ranking'})\n",
    "    y=['overall_ranking']\n",
    "    \n",
    "    # Drop any missing results (DNFs)\n",
    "    data=data.dropna(subset=['overall_ranking'])\n",
    "    \n",
    "    # Ensure there is data to actually predict on\n",
    "    if len(data)==0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Predict on unseen data\n",
    "    from IPython.utils import io\n",
    "    with io.capture_output() as captured:\n",
    "        predictions=pd.DataFrame(fitted_pipe.predict(data[X]),data.index)\n",
    "    observed=data[y]\n",
    "    \n",
    "    # Generate rankings within this bracket\n",
    "    observed=pd.DataFrame(data[y].rank(),data.index)\n",
    "\n",
    "    # Return the ratio of the student score\n",
    "    return pd.Series({\"score\":score(observed,predictions)})\n",
    "\n",
    "# Student solution\n",
    "pipeline_file='pipeline.cloudpickle'\n",
    "\n",
    "def autograde(holdout_data):\n",
    "    # Run prediction on each group\n",
    "    results=holdout_data.groupby([\"event.id\",\"clean_categories.name\"]).apply(evaluate, pipeline_file)\n",
    "\n",
    "    # Display the results, uncomment this for your own display\n",
    "    results.reset_index()['score'].plot.bar();\n",
    "    \n",
    "    # This is the student final grade\n",
    "    print(np.average(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370c5e5-c25e-47a4-84d1-b476a7932997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#autograde(holdout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e4633-8ed7-4b5d-b919-689165270d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say that I want to include in my pipeline the ratio of men to non-men\n",
    "# in the race. I can create that with the following:\n",
    "#sequence_stats=train.groupby(['sequence_id','event.id']).apply(lambda x: sum(x['sex']==1)/len(x)).groupby(['sequence_id']).apply(np.mean)\n",
    "#sequence_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1c622-47fe-4f46-aee8-11ba2e40211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To embed this data in my pipeline, I need to add it to one of the steps - either\n",
    "# a transformer or an estimator - as an object. Then when this gets serialized as\n",
    "# part of the pickle process it will be there for estimation. Let's create a new\n",
    "# transformer which does this, our strategy will be that the __init__ function\n",
    "# will calculate our sequence stats data and store it in the object, while our\n",
    "# transform function will add that to unseen data as a column\n",
    "#import numpy as np\n",
    "\n",
    "#class SequenceSexRatio(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator):\n",
    "    \n",
    "    # this will be called when we first make our pipeline, so we can store things\n",
    "#    def __init__(self, train):\n",
    "        # as we want to visualize this later we have to have something in the train\n",
    "        # attribute, which is the parameter to this function. I'll just make this the \n",
    "        # first row of the data coming in.\n",
    "#        self.train=train.iloc[0]\n",
    "        \n",
    "#        self.sequence_stats=train.groupby(['sequence_id','event.id']).apply(\n",
    "#            lambda x: sum(x['sex']==1)/len(x)).groupby(['sequence_id']).apply(np.mean)\n",
    "        # you need to name a series in order to merge it later\n",
    "#        self.sequence_stats.name='sex_sequence_ratio'\n",
    "    \n",
    "    # this does nothing interesting\n",
    "#    def fit(self, data=None, y=None):\n",
    "#        return self\n",
    "\n",
    "# this will be called when we want to predict our data, since it will transform\n",
    "#    def transform(self, data):\n",
    "        # we can print out some diagnostics here, let's check how many sequences in the\n",
    "        # data we are trying to transform existing in our historical dataset\n",
    "#        print(f'The number of sequences which are also in our historical data are {len(set(data[\"sequence_id\"].unique()).intersection(self.sequence_stats.index))}')\n",
    "        # align on index via a left join\n",
    "#        newdata=pd.merge(data,self.sequence_stats,left_on='sequence_id',right_index=True,how='left')\n",
    "        # set our new sex_sequence_ratio column \n",
    "#        data['SequenceSexRatio']=newdata['sex_sequence_ratio']\n",
    "        # return all of the data to the next stage of the pipeline\n",
    "#        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aade20-f118-4a2e-a42d-2fc37abd421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That assignment should pass the autograder. A more pythonic way to do this, and certainly\n",
    "# the goal of the sklearn team, is to use pipelines, and reuse transformer objects to do the\n",
    "# data cleaning. In sklearn pipelines are made up of a sequence of Transformers with the last\n",
    "# item in the pipeline being an Estimator. You can have Estimators throughout the pipeline\n",
    "# too, creating new features through modeling. For instance, you could use PCA to reduce the\n",
    "# dimensionality of features and then learn on principal components instead.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# We can write a similar model to the above using pipelines and transformers. A good example\n",
    "# would be to first create a transformer for the columns of sex and age, and get rid of\n",
    "# everything else while one hot encoding sex\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cleaner = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"sex\", 'passthrough', ['sex']),\n",
    "        ('age', 'passthrough', ['age']),\n",
    "        ('price', 'passthrough', ['price']),\n",
    "        ('counts.participants.expected', 'passthrough', ['counts.participants.expected']),\n",
    "        ('counts.participants.registered', 'passthrough', ['counts.participants.registered']),\n",
    "        ('bib', 'passthrough', ['bib']),\n",
    "        ('clean_categories_run_count', 'passthrough', ['clean_categories_run_count']),\n",
    "        ('clean_categories_walk_count', 'passthrough', ['clean_categories_walk_count']),\n",
    "        ('location.state', OneHotEncoder(categories=[['AZ', 'CA', 'CO', 'FL', 'GA', 'IL', 'IN', 'MI', 'MN', 'MO', 'NM',\n",
    "       'NC', 'OH', 'OK', 'OR', 'PA', 'TN', 'TX', 'VA', 'WA', 'WV', 'WI']], handle_unknown='ignore'), ['location.state']),\n",
    "        ('event.date.start', OneHotEncoder(sparse=False,categories=[['2017-12-03', '2018-12-02', '2018-01-07', '2018-03-18',\n",
    "       '2018-05-06', '2017-10-01', '2018-10-07', '2017-12-17',\n",
    "       '2018-12-16', '2018-02-04', '2017-06-04', '2017-09-09',\n",
    "       '2017-10-29', '2018-04-29', '2018-05-05', '2018-06-03',\n",
    "       '2018-06-07', '2018-06-21', '2018-06-23', '2018-09-08',\n",
    "       '2018-09-30', '2018-10-14', '2018-10-20', '2018-10-21',\n",
    "       '2018-11-04', '2018-11-22', '2018-12-31', '2018-03-24',\n",
    "       '2018-05-12', '2018-05-31', '2018-06-02', '2018-06-16',\n",
    "       '2018-06-17', '2018-06-30', '2018-07-14', '2018-07-15',\n",
    "       '2018-07-19', '2018-07-21', '2018-07-28', '2018-08-04',\n",
    "       '2018-08-11', '2018-08-16', '2018-08-19', '2018-08-25',\n",
    "       '2018-09-01', '2018-09-29', '2018-10-13', '2018-10-27',\n",
    "       '2018-11-23', '2018-04-14', '2017-12-10', '2018-12-09',\n",
    "       '2017-09-17', '2018-09-16', '2018-02-10', '2017-11-05',\n",
    "       '2018-06-01', '2018-07-27', '2018-09-03', '2018-10-06',\n",
    "       '2018-11-18', '2018-12-22', '2012-08-11', '2013-01-01',\n",
    "       '2013-08-10', '2014-09-27', '2015-01-01', '2015-08-15',\n",
    "       '2015-09-26', '2015-12-12', '2016-03-12', '2016-08-20',\n",
    "       '2016-08-27', '2016-10-02', '2016-10-22', '2016-12-10',\n",
    "       '2017-03-11', '2017-08-26', '2017-09-16', '2017-09-23',\n",
    "       '2017-09-30', '2017-10-28', '2018-03-31', '2018-09-15',\n",
    "       '2018-11-03', '2018-11-17', '2018-12-01', '2018-09-23',\n",
    "       '2018-04-07', '2018-02-17', '2018-01-28', '2018-02-03',\n",
    "       '2016-03-26', '2016-07-16', '2017-03-25', '2017-06-10',\n",
    "       '2017-07-15', '2018-06-09', '2018-06-10', '2018-03-04',\n",
    "       '2018-08-18', '2018-09-22']], handle_unknown='ignore'), ['event.date.start']),\n",
    "        ('result.primary_bracket', 'passthrough', ['result.primary_bracket']),\n",
    "        ('location.city', OneHotEncoder(categories=[['Scottsdale', 'San Francisco', 'San Diego', 'Arcata', 'Denver',\n",
    "       'Tampa', 'Atlanta', 'Chicago', 'Highland Park',\n",
    "       'Elk Grove Village', 'Oak Park', 'Indianapolis', 'Port Huron',\n",
    "       'Flint', 'Swartz Creek', 'Royal Oak', 'Pinconning', 'Roscommon',\n",
    "       'Plymouth', 'Algonac', 'Fenton', 'Detroit', 'Pigeon',\n",
    "       'Bloomfield Hills', 'Rogers City', 'Caseville', 'Montrose',\n",
    "       'Milford', 'Mackinac Island', 'Sault Ste. Marie', 'East Lansing',\n",
    "       'Williamston', 'Howell', 'Minneapolis', 'St. Louis', 'Santa Fe',\n",
    "       'Charlotte', 'Columbus', 'Canal Fulton', 'Brewster', 'Hartville',\n",
    "       'Strasburg', 'North Canton', 'Clinton', 'Cadiz', 'Dalton',\n",
    "       'Alliance', 'Barberton', 'Millersburg', 'Peninsula', 'Bolivar',\n",
    "       'Massillon', 'Canton', 'Akron', 'Oklahoma City', 'Tulsa', 'Norman',\n",
    "       'Edmond', 'Lawton', 'Stillwater', 'Moore', 'Perry', 'Seiling',\n",
    "       'Bethany', 'Elk City', 'Sulphur', 'Yukon', 'Beaverton', 'Salem',\n",
    "       'Albany', 'Philadelphia', 'Nashville', 'Houston', 'Dallas',\n",
    "       'Virginia Beach', 'Fairfax', 'West Point', 'Seattle',\n",
    "       'Parkersburg', 'Milwaukee']], handle_unknown='ignore'), ['location.city']),\n",
    "    ], remainder='drop')\n",
    "​\n",
    "# Then we create a three stage pipeline, where the first step applies the column transformer,\n",
    "# the next step fills our missing values, and the third step is a regression model. But remember,\n",
    "# this isn't a simple regression, we need an ordinal classification. To do this we can wrap\n",
    "# the linear regressor in another class which will transform the regression output. This class\n",
    "# is called the TransformedTargetRegressor, and we can tell it what function we want to apply\n",
    "# to the final output before returning the predictions.\n",
    "​\n",
    "​\n",
    "def evaluation_function(x):\n",
    "    '''Must return a ndarray of the rankings in order, the autograder will then create\n",
    "    a dataframe out of this with x.index as the index. Props to Rachell Calhoun!'''\n",
    "    return pd.Series(x.squeeze()).rank().values\n",
    "​\n",
    "# We can wrap a linear regressor by setting the inverse_func to evaluation_function\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "​\n",
    "#estimators = [('ETR', ExtraTreesRegressor(n_estimators=900, max_depth=4)),('RFR', RandomForestRegressor(n_estimators=50,criterion='squared_error')),('XGBR', XGBRegressor(n_estimators=150, max_depth=1,eta=0.5,subsample=0.1,colsample_bytree=1))]\n",
    "#reg = TransformedTargetRegressor(regressor=StackingRegressor(estimators=estimators, final_estimator=ExtraTreesRegressor(n_estimators=900, max_depth=4)), inverse_func=evaluation_function)\n",
    "​\n",
    "#estimators = [('ETR', ExtraTreesRegressor())]\n",
    "#reg=TransformedTargetRegressor(regressor=ExtraTreesRegressor(n_estimators=900, criterion='squared_error', max_depth=4, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None), inverse_func=evaluation_function)\n",
    "#reg = TransformedTargetRegressor(regressor=LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False), inverse_func=evaluation_function)\n",
    "#reg = TransformedTargetRegressor(regressor=LinearRegression(), inverse_func=evaluation_function)\n",
    "reg = TransformedTargetRegressor(regressor=RandomForestRegressor(n_estimators=50,criterion='squared_error'), inverse_func=evaluation_function)\n",
    "​\n",
    "# Now we can build our three part pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"cleaner\", cleaner),\n",
    "        ('preprocessor', preprocessor),\n",
    "        (\"fix_nans\", SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "        (\"regressor\", reg)\n",
    "    ])\n",
    "​\n",
    "# We can display the pipeline to see what it looks like and get a sense of data flow\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "display(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8648017-1ee8-43d8-ac41-083c78a387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VR = VotingRegressor(estimators=10)\n",
    "#print('Parameters currently in use:\\n')\n",
    "#print(VR.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e9754-1f8e-4960-a63f-ff2da9da1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETR = ExtraTreesRegressor()\n",
    "#print('Parameters currently in use:\\n')\n",
    "#print(ETR.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392e303-db73-4aa3-8f84-77254e433656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFR = RandomForestRegressor()\n",
    "#print('Parameters currently in use:\\n')\n",
    "#print(RFR.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388da30-f386-449f-8722-83a2ad605739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV, cross_validate, KFold, RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c929d9a-2188-45ad-8f1d-3badb324b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_nan = df['clean_categories_walk_count'].isnull().sum()\n",
    "#check_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae2d59-88fc-421c-9565-4178f96facd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478699f-504f-45cd-a437-8b31fdf3ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = train[['sex','age','clean_categories_run_count','clean_categories_walk_count']]\n",
    "#features=features.head(51080)\n",
    "#features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa70dc-5bb4-405f-9279-4c0ffebbdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_20000 = features.iloc[0:19999]\n",
    "#features_20000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755b3d0-4289-4153-8c85-63b41d8d41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_20000 = y.iloc[0:19999]\n",
    "#y_20000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb9052-92b0-4f7b-80e2-3ec16c7ddd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test=train_test_split(features_20000,y_20000,random_state=0,test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010e027-67d2-48eb-840c-dc8fcbf38189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "\n",
    "# identify parameters\n",
    "#param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 400, 500, 600, 700, 800, 900, 1000], \n",
    "#            'max_depth' : [1,2,3,4]} \n",
    "\n",
    "# perform grid search\n",
    "# specify model\n",
    "#ETR = ExtraTreesRegressor()\n",
    "#grid_pipeline_train_ETR = GridSearchCV(estimator=ETR, param_grid=param_grid, cv=folds, verbose=0)\n",
    "#grid_pipeline_train_ETR.fit(X_train, y_train)\n",
    "#grid_pipeline_train_ETR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ece05-2d54-43ca-a327-e4ee8b27425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "\n",
    "# identify parameters\n",
    "#param_grid = {'verbose': [True,False]} \n",
    "\n",
    "# perform grid search\n",
    "# specify model\n",
    "#estimators = [('ETR', ExtraTreesRegressor(n_estimators=900, max_depth=4))]\n",
    "#VR = VotingRegressor(estimators=estimators)\n",
    "#grid_pipeline_train_VR = GridSearchCV(estimator=VR, param_grid=param_grid, cv=folds, verbose=0)\n",
    "#grid_pipeline_train_VR.fit(X_train, y_train)\n",
    "#grid_pipeline_train_VR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bdb82-5ac6-4ed7-ad21-b8913bbd32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#create a cross-validation scheme\n",
    "#folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "\n",
    "# identify parameters\n",
    "#param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 400, 500, 600, 700, 800, 900, 1000], \n",
    "#              'criterion' : ['squared_error','poisson','friedman_mse','absolute_error']}  \n",
    "\n",
    "# perform grid search\n",
    "# specify model\n",
    "#RFR = RandomForestRegressor()\n",
    "#grid_pipeline_train_RFR = GridSearchCV(estimator=RFR, param_grid=param_grid, cv=folds, verbose=0)\n",
    "#grid_pipeline_train_RFR.fit(X_train, y_train)\n",
    "#grid_pipeline_train_RFR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a73ed-3d04-4fe4-8269-cd25b7dffc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "\n",
    "# identify parameters\n",
    "#param_grid = {'cv': ['int','cross-validation generator','iterable','prefit', None]} \n",
    "\n",
    "# perform grid search\n",
    "# specify model\n",
    "#estimators = [('ETR', ExtraTreesRegressor(n_estimators=900, max_depth=4)),('RFR', RandomForestRegressor(n_estimators=50,criterion='squared_error')),('XGBR', XGBRegressor(n_estimators=150, max_depth=1,eta=0.5,subsample=0.1,colsample_bytree=1))]\n",
    "#SR = StackingRegressor(estimators=estimators, final_estimator=XGBRegressor(n_estimators=150, max_depth=1,eta=0.5,subsample=0.1,colsample_bytree=1))\n",
    "#grid_pipeline_train_SR = GridSearchCV(estimator=SR, param_grid=param_grid, cv=folds,verbose=0)\n",
    "#grid_pipeline_train_SR.fit(X_train, y_train)\n",
    "#grid_pipeline_train_SR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf2a54-6685-48da-b9e1-189892e4ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#create a cross-validation scheme\n",
    "#folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "​\n",
    "# identify parameters\n",
    "#param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "#              'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "#              'eta':[0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#              'subsample':[0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#               'colsample_bytree':[0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]}  \n",
    "​\n",
    "# perform grid search\n",
    "# specify model\n",
    "#RFR = XGBRegressor()\n",
    "#grid_pipeline_train_XGBR = GridSearchCV(estimator=XGBR, param_grid=param_grid, cv=folds, verbose=0)\n",
    "#grid_pipeline_train_XGBR.fit(X_train, y_train)\n",
    "#grid_pipeline_train_XGBR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c7638-af99-47d7-9797-271263b5b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#create a cross-validation scheme\n",
    "#folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "​\n",
    "# identify parameters\n",
    "#param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "#              'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "#              'eta':[0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#              'subsample':[0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#               'colsample_bytree':[0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]}  \n",
    "​\n",
    "# perform grid search\n",
    "# specify model\n",
    "#RFR = XGBRegressor()\n",
    "#grid_pipeline_train_XGBR = GridSearchCV(estimator=XGBR, param_grid=param_grid, cv=folds, verbose=0)\n",
    "#grid_pipeline_train_XGBR.fit(X_train, y_train)\n",
    "#grid_pipeline_train_XGBR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f0d61-5306-48b4-9992-25b2e6447d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cf9d1-5225-41cd-af18-949a75e62dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1 = y.head(3453)\n",
    "#y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5794b8a-856b-4caf-bbe8-edbcb947f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pipe = pipe.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527eb916-c0aa-4dc9-82d0-2f2a6d5d5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to fit out pipeline, this will just call the transform() and fit()\n",
    "# functions of the objects in the pipeline, but will not create new objects.\n",
    "#fitted_pipe=pipe.fit(train, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8f03a-1f90-4863-ad77-8ce17a908367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92a5f9-f66e-4361-9ac5-e670cf9a6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074fe2d-a8ec-4687-a0df-57c6da308a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can take that final regression object and observe the coefficients\n",
    "# to verify that we have four, two for sex, one for sexsequenceratio, and\n",
    "# one age\n",
    "#fitted_pipe.steps[-1][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97402ee-124c-49d8-bf35-2c2c701ad5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805f3ba-ceab-4e39-a04b-a0b1a26b8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y2 = y.head(4444)\n",
    "#y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c090e-d646-434a-8b27-daf258021aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1 = test[:3543]\n",
    "#test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c430a-b43d-40b7-a9cf-91023e6cb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can now try this on unseen data\n",
    "#fitted_pipe.score(test1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d448caf-36bf-46a0-8ee1-293ccfdb6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "#cv = KFold(n_splits=5)\n",
    "# evaluate model\n",
    "#scores = cross_val_score(fitted_pipe, train, y, scoring='r2', cv=cv)\n",
    "# force scores to be positive\n",
    "#scores = abs(scores)\n",
    "#print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c4d589-8c85-43f9-af82-6b93f813347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = cross_val_score(fitted_pipe, train, y, scoring='r2', cv=cv)\n",
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0c617-0d56-42c1-af25-f628939446e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitted_pipe = pipe.fit(train, y)\n",
    "\n",
    "# And we can save it to an output file\n",
    "cloudpickle.dump(fitted_pipe, open('pipeline.cloudpickle','wb'))\n",
    "\n",
    "pipeline_file='pipeline.cloudpickle'\n",
    "\n",
    "# Run prediction on each group\n",
    "results=holdout_data.groupby([\"event.id\",\"clean_categories.name\"]).apply(evaluate, pipeline_file)\n",
    "\n",
    "# Display the results, uncomment this for your own display\n",
    "results.reset_index()['score'].plot.bar();\n",
    "\n",
    "# This is the student final grade\n",
    "np.average(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad62d43-b283-45d0-adfc-2d8c15096195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#marketing_cleaned = (marketing.pipe(drop_missing).pipe(remove_outliers, 'Salary').pipe(to_category))\n",
    "#cols = df.columns\n",
    "\n",
    "#1. Drop Rows with Null Columns\n",
    "#def drop_nulls(df, cols):\n",
    "#    df.dropna(subset=cols, inplace=True)\n",
    "#    return df\n",
    "\n",
    "#2. Fill Null Columns with Av Values\n",
    "#def fill_vals(df, cols):\n",
    "#    for i in cols:\n",
    "#        av=df[i].mean()\n",
    "#        df[i].fillna(av, inplace=True)\n",
    "#        return df\n",
    "    \n",
    "#3. Replace Strings with numbers and convert type\n",
    "#def replace_strings(df, cols):\n",
    "#    df.loc[df.query(\"'age' >= 119\").index, 'age']=np.nan\n",
    "#    df.loc[df.query(\"'age' <= 18\").index, 'age']=np.nan\n",
    "#    df['age']=df['age'].replace(np.nan, 38)\n",
    "#    df['price']=df['price'].replace(np.nan,49.0)\n",
    "#    df['bib']=df['bib'].replace(np.nan, 5202.0)\n",
    "#    df['counts.participants.expected']=df['counts.participants.expected'].replace(['150-300'],['300'])\n",
    "#    df['event.results_certificate']=df['event.results_certificate'].replace([False, 'False', '1', 1],[0,0,1,1])\n",
    "#    df['result.primary_bracket']=df['result.primary_bracket'].replace(['WHEELCHAIR','nan','Male 25-29','Male 40-44','Female 25-29','Female 20-24','wheelchair'],['Wheelchair','Overall','Male','Male','Female','Female','Wheelchair'])\n",
    "#    for i in cols:\n",
    "        #df[i].replace('3+', 4, inplace=True)\n",
    "#        df[i] = pd.to_numeric(df[i])\n",
    "        \n",
    "#        return df\n",
    "    \n",
    "#null_cols = ['Gender', 'Married', 'Dependents', 'Credit_History', 'Self_Employed', 'Loan_Amount_Term']\n",
    "#av_cols = ['LoanAmount']\n",
    "#rp_cols = ['Dependents']\n",
    "\n",
    "#df = df.pipe(drop_nulls, null_cols).pipe(fill_vals, av_cols).pipe(replace_strings, rp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12472cf9-7315-4efa-afe0-fe55bbe0fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = clf.predict(X)\n",
    "#import numpy as np\n",
    "#def my_custom_loss_func(y_true, y_pred):\n",
    "#    diff = np.abs(y_true - y_pred).max()\n",
    "#    return np.log1p(diff)\n",
    "\n",
    "# score will negate the return value of my_custom_loss_func,\n",
    "# which will be np.log(2), 0.693, given the values for X\n",
    "# and y defined below.\n",
    "#score = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "#X = train\n",
    "\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "#clf = clf.fit(X, y)\n",
    "#my_custom_loss_func(y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f311e8-892a-4ea2-bfcc-1f5a990d52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score(clf, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
